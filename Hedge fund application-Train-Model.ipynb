{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.191:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1575249527713)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object SparkSessionCreator\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object SparkSessionCreator {\n",
    "\n",
    "  def sparkSessionCreate(): SparkSession = {\n",
    "\n",
    "    SparkSession\n",
    "      .builder()\n",
    "      .master(\"local[*]\")\n",
    "      .appName(\"Hedge-fund-application-realtime-analysis\")\n",
    "      .getOrCreate()\n",
    "\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.sql.functions.lit\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions.lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object DataSourcer\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object DataSourcer {\n",
    "\n",
    "  def rawTrainData(sparkSession: SparkSession): DataFrame = {\n",
    "         \n",
    "      // load train data from local\n",
    "        sparkSession.read.option(\"header\", \"true\").csv(\"./data/train.csv\")\n",
    "    }\n",
    "\n",
    "  def rawTestData(sparkSession: SparkSession): DataFrame = {\n",
    "\n",
    "    // load train data from local\n",
    "    // populate label col - not included in raw test data\n",
    "    sparkSession.read.option(\"header\", \"true\").csv(\"./data/test.csv\")\n",
    "      .withColumn(\"Open\", lit(\"0\"))\n",
    "\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object DataCleaner\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object DataCleaner {\n",
    "\n",
    "  // function to produce a clean data frame from a raw data frame\n",
    "  def cleanData(dataFrame: DataFrame): DataFrame = {\n",
    "\n",
    "    // def function to format data correctly\n",
    "    def formatData(dataFrame: DataFrame): DataFrame = {\n",
    "\n",
    "      dataFrame\n",
    "        .withColumn(\"Open\", dataFrame(\"Open\").cast(\"Double\"))\n",
    "        .withColumn(\"High\", dataFrame(\"High\").cast(\"Double\"))\n",
    "        .withColumn(\"Low\", dataFrame(\"Low\").cast(\"Double\"))\n",
    "        .withColumn(\"Close\", dataFrame(\"Close\").cast(\"Double\"))\n",
    "        .withColumn(\"Volume\", dataFrame(\"Volume\").cast(\"Double\"))\n",
    "        .withColumnRenamed(\"Open\", \"label\")\n",
    "\n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "    // format raw data\n",
    "    val outputData = formatData(dataFrame)\n",
    "\n",
    "\n",
    "    // return cleaned data frame\n",
    "    outputData\n",
    "\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object OutputSaver\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object OutputSaver {\n",
    "\n",
    "  // function to save a fitted pipeline\n",
    "  def pipelineSaver(pipelineModel: PipelineModel): Unit = {\n",
    "\n",
    "    pipelineModel\n",
    "      .write\n",
    "      .overwrite()\n",
    "      .save(\"./pipelines/fitted-pipeline\")\n",
    "\n",
    "  }\n",
    "\n",
    "  // function to save predictions\n",
    "//   def predictionsSaver(dataFrame: DataFrame): Unit = {\n",
    "\n",
    "//     dataFrame\n",
    "//       .select(\"PassengerId\", \"prediction\")\n",
    "//       .withColumnRenamed(\"prediction\", \"Survived\")\n",
    "//       .write\n",
    "//       .mode(saveMode = SaveMode.Overwrite)\n",
    "//       .csv(path = \"./predictions/predictions_csv\")\n",
    "\n",
    "//   }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "139: error: not found: value rf",
     "output_type": "error",
     "traceback": [
      "<console>:139: error: not found: value rf",
      "             val gbtModel = rf.fit(df_train)",
      "                            ^",
      "<console>:140: error: not found: value rfrModel",
      "             val predictions = rfrModel.transform(df_test)",
      "                               ^",
      ""
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.regression.GeneralizedLinearRegression\n",
    "import org.apache.spark.ml.regression.{RandomForestRegressionModel, RandomForestRegressor}\n",
    "import org.apache.spark.ml.regression.{GBTRegressionModel, GBTRegressor}\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorAssembler, VectorIndexer}\n",
    "\n",
    "object MachineLearning {\n",
    "\n",
    "  def pipelineFit(dataFrame: DataFrame): Unit = {\n",
    "      \n",
    "    // define feature vector assembler\n",
    "    val featureAssembler = new VectorAssembler()\n",
    "      .setInputCols(Array[String](\n",
    "          \"High\",\n",
    "          \"Low\",\n",
    "          \"Close\",\n",
    "          \"Volume\"\n",
    "        )\n",
    "      )\n",
    "      .setOutputCol(\"features\")\n",
    "\n",
    "   val Array(trainingData, testData) = dataFrame.randomSplit(Array(0.7, 0.3))  \n",
    "      \n",
    "    var df_train = featureAssembler.transform(trainingData)\n",
    "    var df_test =  featureAssembler.transform(testData) \n",
    "      \n",
    "//     //LINEAR REGRESSION  \n",
    "      \n",
    "//     //Model Creation  \n",
    "//     val lr = new LinearRegression()\n",
    "  \n",
    "//     // Fit the model\n",
    "//     val lrModel = lr.fit(df)\n",
    "      \n",
    "    \n",
    "//     //GeneralizedLinearRegression\n",
    "      \n",
    "//     //Model Creation\n",
    "//     val glr = new GeneralizedLinearRegression()\n",
    "//       .setFamily(\"gaussian\")\n",
    "//       .setLink(\"identity\")\n",
    "//       .setMaxIter(10)\n",
    "//       .setRegParam(0.3)\n",
    "      \n",
    "//     // Fit the Model\n",
    "//     val glrModel = glr.fit(df)\n",
    "      \n",
    "//         val featureIndexer = new VectorIndexer()\n",
    "//           .setInputCol(\"features\")\n",
    "//           .setOutputCol(\"label\")\n",
    "//           .setMaxCategories(4)\n",
    "//           .fit(data)\n",
    "      \n",
    "//       val rf = new RandomForestRegressor()\n",
    "//           .setLabelCol(\"label\")\n",
    "//           .setFeaturesCol(\"features\")\n",
    "      \n",
    "//       val rfrModel = rf.fit(df_train)\n",
    "//       val predictions = rfrModel.transform(df_test)\n",
    "//       predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "      \n",
    "//       // Select (prediction, true label) and compute test error.\n",
    "//         val evaluator = new RegressionEvaluator()\n",
    "//           .setLabelCol(\"label\")\n",
    "//           .setPredictionCol(\"prediction\")\n",
    "//           .setMetricName(\"rmse\")\n",
    "//         val rmse = evaluator.evaluate(predictions)\n",
    "//         println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")\n",
    "      \n",
    "      \n",
    "      //////////////////////////////////////////////////////////////////////////////////////\n",
    "      \n",
    "      val gbt = new GBTRegressor()\n",
    "              .setLabelCol(\"label\")\n",
    "              .setFeaturesCol(\"features\")\n",
    "              .setMaxIter(10)\n",
    "      \n",
    "      val gbtModel = rf.fit(df_train)\n",
    "      val predictions = rfrModel.transform(df_test)\n",
    "      predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "      \n",
    "      // Select (prediction, true label) and compute test error.\n",
    "        val evaluator = new RegressionEvaluator()\n",
    "          .setLabelCol(\"label\")\n",
    "          .setPredictionCol(\"prediction\")\n",
    "          .setMetricName(\"rmse\")\n",
    "        val rmse = evaluator.evaluate(predictions)\n",
    "        println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")\n",
    "      \n",
    "      ///////////////////\n",
    "      \n",
    "      // chain indexers and forest in a Pipeline\n",
    "            val pipeline = new Pipeline()\n",
    "              .setStages(\n",
    "                Array(\n",
    "                  featureAssembler,\n",
    "                  gbt\n",
    "                )\n",
    "              )\n",
    "      \n",
    "          val pipelineModel_gbt = pipeline.fit(dataFrame)\n",
    "\n",
    "          OutputSaver.pipelineSaver(pipelineModel = pipelineModel_gbt)\n",
    "      \n",
    "      \n",
    "        }\n",
    "\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.LinearRegression\n",
       "import org.apache.spark.ml.regression.GeneralizedLinearRegression\n",
       "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorAssembler, VectorIndexer}\n",
       "defined object MachineLearning\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    //LINEAR REGRESSION  \n",
    "    println(\"Linear Regression\")\n",
    "    println(s\"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}\")\n",
    "\n",
    "    // Summarize the model over the training set and print out some metrics\n",
    "    val trainingSummary_lr = lrModel.summary\n",
    "    println(s\"numIterations: ${trainingSummary_lr.totalIterations}\")\n",
    "    println(s\"objectiveHistory: [${trainingSummary_lr.objectiveHistory.mkString(\",\")}]\")\n",
    "    trainingSummary_lr.residuals.show()\n",
    "    println(s\"RMSE: ${trainingSummary_lr.rootMeanSquaredError}\")\n",
    "    println(s\"r2: ${trainingSummary_lr.r2}\")\n",
    "      \n",
    "    println()\n",
    "    println()  \n",
    "      \n",
    "   ////////////////////////////////////////////////////////////////////////////////////////\n",
    "    \n",
    "//     println(\" Generalized Linear Regression\")\n",
    "//     println(s\"Coefficients: ${glrModel.coefficients} Intercept: ${glrModel.intercept}\")\n",
    "\n",
    "//     // Summarize the model over the training set and print out some metrics\n",
    "//     val trainingSummary_glr = glrModel.summary\n",
    "//     //println(s\"numIterations: ${trainingSummary_glr.totalIterations}\")\n",
    "//     println(s\"objectiveHistory: [${trainingSummary_glr.objectiveHistory.mkString(\",\")}]\")\n",
    "//     trainingSummary_glr.residuals.show()\n",
    "//     println(s\"RMSE: ${trainingSummary_glr.rootMeanSquaredError}\")\n",
    "//     println(s\"r2: ${trainingSummary_glr.r2}\")\n",
    "      \n",
    "//     println()\n",
    "//     println()     \n",
    "      \n",
    "      \n",
    "\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@64e30ad0\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    // create spark session\n",
    "    val spark = SparkSessionCreator.sparkSessionCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawTrainData: org.apache.spark.sql.DataFrame = [Open: string, High: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    // train data\n",
    "    val rawTrainData = DataSourcer.rawTrainData(sparkSession = spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleanTrainData: org.apache.spark.sql.DataFrame = [label: double, High: double ... 3 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    // clean train data\n",
    "    val cleanTrainData = DataCleaner.cleanData(dataFrame = rawTrainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+----------+\n",
      "|    label|     High|      Low|    Close|    Volume|\n",
      "+---------+---------+---------+---------+----------+\n",
      "|28.459999|28.994286|28.338572|28.655714| 7.38143E7|\n",
      "|    28.73|28.811428|28.395714|28.558571|1.062145E8|\n",
      "|28.891428|28.967142|28.118572|28.138571|1.164408E8|\n",
      "|28.422857|28.774286|27.964285|28.032858| 1.78815E8|\n",
      "|28.202858|28.425714|28.038572|28.068571|1.121799E8|\n",
      "|28.528572|28.554285|27.182858|27.617144|2.067212E8|\n",
      "|27.617144|27.681429|26.954287|26.992857|1.786897E8|\n",
      "|27.051428|27.478571|26.957144|27.124287|1.725997E8|\n",
      "|27.325714|28.308571|27.187143|28.257143|1.711955E8|\n",
      "|     28.5|28.528572|28.017143|28.061428|1.224174E8|\n",
      "|28.254286|28.285715|27.632856|27.809999|1.074437E8|\n",
      "|    27.91|28.204287|27.508572|28.139999|1.239476E8|\n",
      "|27.975714|28.215714|27.610001|27.738571|1.048649E8|\n",
      "|27.871429|28.071428|27.792856|27.861429| 8.82462E7|\n",
      "|27.751429|27.857143|27.285715|27.408571| 9.72097E7|\n",
      "|27.595715|27.928572|27.514286|27.918571|1.521926E8|\n",
      "|28.007143|28.535715|27.952858|28.318571|1.529766E8|\n",
      "|28.491428|28.692858|28.379999|28.622858| 8.73789E7|\n",
      "|28.742857|28.911428|28.687143|28.871429| 8.63814E7|\n",
      "|29.078571|29.907143|29.049999|29.862858|1.252223E8|\n",
      "+---------+---------+---------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanTrainData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+--------------------+\n",
      "|       prediction|    label|            features|\n",
      "+-----------------+---------+--------------------+\n",
      "|28.46461044543444|27.871429|[28.071428,27.792...|\n",
      "|28.46461044543444|27.984285|[28.085714,27.751...|\n",
      "|28.46461044543444|28.059999|[28.214285,27.821...|\n",
      "|28.46461044543444|28.202858|[28.425714,28.038...|\n",
      "|28.46461044543444|28.254286|[28.285715,27.632...|\n",
      "+-----------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 1.1965864737939682\n"
     ]
    }
   ],
   "source": [
    "    // fitted pipeline\n",
    "MachineLearning.pipelineFit(dataFrame = cleanTrainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
