{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.0.191:4041\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1575251419409)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object SparkSessionCreator\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object SparkSessionCreator {\n",
    "\n",
    "  def sparkSessionCreate(): SparkSession = {\n",
    "\n",
    "    SparkSession\n",
    "      .builder()\n",
    "      .master(\"local[*]\")\n",
    "      .appName(\"Hedge-fund-application-realtime-analysis\")\n",
    "      .getOrCreate()\n",
    "\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.sql.functions.lit\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions.lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object DataSourcer\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object DataSourcer {\n",
    "\n",
    "  def rawTrainData(sparkSession: SparkSession): DataFrame = {\n",
    "         \n",
    "      // load train data from local\n",
    "        sparkSession.read.option(\"header\", \"true\").csv(\"./data/train.csv\")\n",
    "    }\n",
    "\n",
    "  def rawTestData(sparkSession: SparkSession): DataFrame = {\n",
    "\n",
    "    // load train data from local\n",
    "    // populate label col - not included in raw test data\n",
    "    sparkSession.read.option(\"header\", \"true\").csv(\"./data/test.csv\")\n",
    "      .withColumn(\"Open\", lit(\"0\"))\n",
    "\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object DataCleaner\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object DataCleaner {\n",
    "\n",
    "  // function to produce a clean data frame from a raw data frame\n",
    "  def cleanData(dataFrame: DataFrame): DataFrame = {\n",
    "\n",
    "    // def function to format data correctly\n",
    "    def formatData(dataFrame: DataFrame): DataFrame = {\n",
    "\n",
    "      dataFrame\n",
    "        .withColumn(\"Open\", dataFrame(\"Open\").cast(\"Double\"))\n",
    "        .withColumn(\"High\", dataFrame(\"High\").cast(\"Double\"))\n",
    "        .withColumn(\"Low\", dataFrame(\"Low\").cast(\"Double\"))\n",
    "        .withColumn(\"Close\", dataFrame(\"Close\").cast(\"Double\"))\n",
    "        .withColumn(\"Volume\", dataFrame(\"Volume\").cast(\"Double\"))\n",
    "        .withColumnRenamed(\"Open\", \"label\")\n",
    "\n",
    "    }\n",
    "\n",
    "    \n",
    "\n",
    "    // format raw data\n",
    "    val outputData = formatData(dataFrame)\n",
    "\n",
    "\n",
    "    // return cleaned data frame\n",
    "    outputData\n",
    "\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.PipelineModel\n",
       "import org.apache.spark.sql.{DataFrame, SaveMode}\n",
       "defined object OutputSaver\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.PipelineModel\n",
    "import org.apache.spark.sql.{DataFrame, SaveMode}\n",
    "object OutputSaver {\n",
    "\n",
    "  // function to save a fitted pipeline\n",
    "  def pipelineSaver(pipelineModel: PipelineModel): Unit = {\n",
    "\n",
    "    pipelineModel\n",
    "      .write\n",
    "      .overwrite()\n",
    "      .save(\"./pipelines/fitted-pipeline\")\n",
    "\n",
    "  }\n",
    "\n",
    "  // function to save predictions\n",
    "  def predictionsSaver(dataFrame: DataFrame): Unit = {\n",
    "\n",
    "    dataFrame\n",
    "      .select(\"High\", \"Low\", \"Close\", \"Volume\", \"prediction\")\n",
    "      .write\n",
    "      .mode(saveMode = SaveMode.Overwrite)\n",
    "      .csv(path = \"./predictions/predictions_csv\")\n",
    "\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineModel}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "115: error: not found: type GBTRegressor",
     "output_type": "error",
     "traceback": [
      "<console>:115: error: not found: type GBTRegressor",
      "             val gbt = new GBTRegressor()",
      "                           ^",
      "<console>:120: error: not found: value rf",
      "             val gbtModel = rf.fit(df_train)",
      "                            ^",
      "<console>:121: error: not found: value rfrModel",
      "             val predictions = rfrModel.transform(df_test)",
      "                               ^",
      "<console>:135: error: not found: type Pipeline",
      "                   val pipeline = new Pipeline()",
      "                                      ^",
      ""
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.regression.GeneralizedLinearRegression\n",
    "import org.apache.spark.ml.regression.{RandomForestRegressionModel, RandomForestRegressor}\n",
    "import org.apache.spark.ml.regression.{GBTRegressionModel, GBTRegressor}\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorAssembler, VectorIndexer}\n",
    "\n",
    "object MachineLearning {\n",
    "\n",
    "  def pipelineFit(dataFrame: DataFrame): Unit = {\n",
    "      \n",
    "    // define feature vector assembler\n",
    "    val featureAssembler = new VectorAssembler()\n",
    "      .setInputCols(Array[String](\n",
    "          \"High\",\n",
    "          \"Low\",\n",
    "          \"Close\",\n",
    "          \"Volume\"\n",
    "        )\n",
    "      )\n",
    "      .setOutputCol(\"features\")\n",
    "\n",
    "   val Array(trainingData, testData) = dataFrame.randomSplit(Array(0.7, 0.3))  \n",
    "      \n",
    "    var df_train = featureAssembler.transform(trainingData)\n",
    "    var df_test =  featureAssembler.transform(testData) \n",
    "      \n",
    "//     //LINEAR REGRESSION  \n",
    "      \n",
    "//     //Model Creation  \n",
    "//     val lr = new LinearRegression()\n",
    "  \n",
    "//     // Fit the model\n",
    "//     val lrModel = lr.fit(df)\n",
    "      \n",
    "    \n",
    "//     //GeneralizedLinearRegression\n",
    "      \n",
    "//     //Model Creation\n",
    "//     val glr = new GeneralizedLinearRegression()\n",
    "//       .setFamily(\"gaussian\")\n",
    "//       .setLink(\"identity\")\n",
    "//       .setMaxIter(10)\n",
    "//       .setRegParam(0.3)\n",
    "      \n",
    "//     // Fit the Model\n",
    "//     val glrModel = glr.fit(df)\n",
    "      \n",
    "//         val featureIndexer = new VectorIndexer()\n",
    "//           .setInputCol(\"features\")\n",
    "//           .setOutputCol(\"label\")\n",
    "//           .setMaxCategories(4)\n",
    "//           .fit(data)\n",
    "      \n",
    "//       val rf = new RandomForestRegressor()\n",
    "//           .setLabelCol(\"label\")\n",
    "//           .setFeaturesCol(\"features\")\n",
    "      \n",
    "//       val rfrModel = rf.fit(df_train)\n",
    "//       val predictions = rfrModel.transform(df_test)\n",
    "//       predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "      \n",
    "//       // Select (prediction, true label) and compute test error.\n",
    "//         val evaluator = new RegressionEvaluator()\n",
    "//           .setLabelCol(\"label\")\n",
    "//           .setPredictionCol(\"prediction\")\n",
    "//           .setMetricName(\"rmse\")\n",
    "//         val rmse = evaluator.evaluate(predictions)\n",
    "//         println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")\n",
    "      \n",
    "      \n",
    "      //////////////////////////////////////////////////////////////////////////////////////\n",
    "      \n",
    "      val gbt = new GBTRegressor()\n",
    "              .setLabelCol(\"label\")\n",
    "              .setFeaturesCol(\"features\")\n",
    "              .setMaxIter(10)\n",
    "      \n",
    "      val gbtModel = rf.fit(df_train)\n",
    "      val predictions = rfrModel.transform(df_test)\n",
    "      predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
    "      \n",
    "      // Select (prediction, true label) and compute test error.\n",
    "        val evaluator = new RegressionEvaluator()\n",
    "          .setLabelCol(\"label\")\n",
    "          .setPredictionCol(\"prediction\")\n",
    "          .setMetricName(\"rmse\")\n",
    "        val rmse = evaluator.evaluate(predictions)\n",
    "        println(s\"Root Mean Squared Error (RMSE) on test data = $rmse\")\n",
    "      \n",
    "      ///////////////////\n",
    "      \n",
    "      // chain indexers and forest in a Pipeline\n",
    "            val pipeline = new Pipeline()\n",
    "              .setStages(\n",
    "                Array(\n",
    "                  featureAssembler,\n",
    "                  gbt\n",
    "                )\n",
    "              )\n",
    "      \n",
    "          val pipelineModel_gbt = pipeline.fit(dataFrame)\n",
    "\n",
    "          OutputSaver.pipelineSaver(pipelineModel = pipelineModel_gbt)\n",
    "      \n",
    "      \n",
    "        }\n",
    "\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@64e30ad0\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    // create spark session\n",
    "    val spark = SparkSessionCreator.sparkSessionCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawTrainData: org.apache.spark.sql.DataFrame = [Open: string, High: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    // train data\n",
    "    val rawTrainData = DataSourcer.rawTestData(sparkSession = spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleanTrainData: org.apache.spark.sql.DataFrame = [label: double, High: double ... 3 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    // clean train data\n",
    "    val cleanTrainData = DataCleaner.cleanData(dataFrame = rawTestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+----------+\n",
      "|    label|     High|      Low|    Close|    Volume|\n",
      "+---------+---------+---------+---------+----------+\n",
      "|28.459999|28.994286|28.338572|28.655714| 7.38143E7|\n",
      "|    28.73|28.811428|28.395714|28.558571|1.062145E8|\n",
      "|28.891428|28.967142|28.118572|28.138571|1.164408E8|\n",
      "|28.422857|28.774286|27.964285|28.032858| 1.78815E8|\n",
      "|28.202858|28.425714|28.038572|28.068571|1.121799E8|\n",
      "|28.528572|28.554285|27.182858|27.617144|2.067212E8|\n",
      "|27.617144|27.681429|26.954287|26.992857|1.786897E8|\n",
      "|27.051428|27.478571|26.957144|27.124287|1.725997E8|\n",
      "|27.325714|28.308571|27.187143|28.257143|1.711955E8|\n",
      "|     28.5|28.528572|28.017143|28.061428|1.224174E8|\n",
      "|28.254286|28.285715|27.632856|27.809999|1.074437E8|\n",
      "|    27.91|28.204287|27.508572|28.139999|1.239476E8|\n",
      "|27.975714|28.215714|27.610001|27.738571|1.048649E8|\n",
      "|27.871429|28.071428|27.792856|27.861429| 8.82462E7|\n",
      "|27.751429|27.857143|27.285715|27.408571| 9.72097E7|\n",
      "|27.595715|27.928572|27.514286|27.918571|1.521926E8|\n",
      "|28.007143|28.535715|27.952858|28.318571|1.529766E8|\n",
      "|28.491428|28.692858|28.379999|28.622858| 8.73789E7|\n",
      "|28.742857|28.911428|28.687143|28.871429| 8.63814E7|\n",
      "|29.078571|29.907143|29.049999|29.862858|1.252223E8|\n",
      "+---------+---------+---------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanTrainData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+--------------------+\n",
      "|       prediction|    label|            features|\n",
      "+-----------------+---------+--------------------+\n",
      "|28.49223630005421|27.051428|[27.478571,26.957...|\n",
      "|28.49223630005421|27.325714|[28.308571,27.187...|\n",
      "|28.49223630005421|27.481428|[28.0,27.328571,2...|\n",
      "|28.52051627223603|27.984285|[28.085714,27.751...|\n",
      "|28.49223630005421|28.301428|[28.805714,27.928...|\n",
      "+-----------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 1.1792396786561206\n"
     ]
    }
   ],
   "source": [
    "    // fitted pipeline\n",
    "MachineLearning.pipelineFit(dataFrame = cleanTrainData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@2017e003\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// create spark session\n",
    "    val spark = SparkSessionCreator.sparkSessionCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rawTestData: org.apache.spark.sql.DataFrame = [High: string, Low: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    // train data\n",
    "    val rawTestData = DataSourcer.rawTestData(sparkSession = spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleanTestData: org.apache.spark.sql.DataFrame = [High: double, Low: double ... 3 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    // clean train data\n",
    "    val cleanTestData = DataCleaner.cleanData(dataFrame = rawTestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fittedPipeline: org.apache.spark.ml.PipelineModel = pipeline_688501a8bab7\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    // load fitted pipeline\n",
    "    val fittedPipeline = PipelineModel.load(\"./pipelines/fitted-pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [High: double, Low: double ... 5 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    // make predictions\n",
    "    val predictions = fittedPipeline.transform(dataset = cleanTestData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------+-----+--------------------+------------------+\n",
      "|      High|       Low|     Close|   Volume|label|            features|        prediction|\n",
      "+----------+----------+----------+---------+-----+--------------------+------------------+\n",
      "|112.470001|111.389999|    111.57| 2.7194E7|  0.0|[112.470001,111.3...|111.17173939126329|\n",
      "|112.029999|    110.07|111.459999|2.85288E7|  0.0|[112.029999,110.0...|110.95592491018374|\n",
      "|112.199997|110.269997|110.519997|3.61623E7|  0.0|[112.199997,110.2...|110.95592491018374|\n",
      "|110.940002|109.029999|109.489998|3.70869E7|  0.0|[110.940002,109.0...|110.95592491018374|\n",
      "|110.089996|108.849998|109.900002| 2.6528E7|  0.0|[110.089996,108.8...|109.22883343362443|\n",
      "|110.029999|    108.25|109.110001|3.43245E7|  0.0|[110.029999,108.2...|108.50450191703264|\n",
      "|110.360001|109.190002|109.949997|2.61955E7|  0.0|[110.360001,109.1...|109.22883343362443|\n",
      "|111.190002|109.160004|111.029999|2.99987E7|  0.0|[111.190002,109.1...|110.95592491018374|\n",
      "|    112.43|110.599998|112.120003|2.70683E7|  0.0|[112.43,110.59999...|111.79460701581327|\n",
      "|114.699997|112.309998|113.949997|3.44026E7|  0.0|[114.699997,112.3...|112.22806835078302|\n",
      "|     115.0|112.489998|113.300003|2.63744E7|  0.0|[115.0,112.489998...|113.78186176784179|\n",
      "|115.919998|    113.75|115.190002|4.37338E7|  0.0|[115.919998,113.7...|115.41539780463151|\n",
      "|116.199997|114.980003|115.190002|3.40318E7|  0.0|[116.199997,114.9...|115.41539780463151|\n",
      "|116.730003|115.230003|    115.82|4.65245E7|  0.0|[116.730003,115.2...|115.41539780463151|\n",
      "|     116.5|115.650002|115.970001|4.43511E7|  0.0|[116.5,115.650002...|115.41539780463151|\n",
      "|117.379997|    115.75|116.639999|2.77794E7|  0.0|[117.379997,115.7...|115.41539780463151|\n",
      "|     117.5|    116.68|116.949997| 2.1425E7|  0.0|[117.5,116.68,116...|  116.909890616556|\n",
      "|117.400002|116.779999|117.059998|2.37832E7|  0.0|[117.400002,116.7...|117.42732305347985|\n",
      "|116.510002|115.639999|116.290001|2.60859E7|  0.0|[116.510002,115.6...|115.41539780463151|\n",
      "|116.519997|115.589996|116.519997|1.42495E7|  0.0|[116.519997,115.5...|115.41539780463151|\n",
      "+----------+----------+----------+---------+-----+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "OutputSaver.predictionsSaver(dataFrame = predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
